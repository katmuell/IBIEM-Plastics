---
title: "Demultiplexing and DADA2"
author: "Katherine Mueller"
date: "3/3/2021"
output: html_document
---

# Load Libraries
```{r}
library(readr)
library(fs)
library(dplyr)
library(tibble)
library(Biostrings)
library(dada2)
library(stringr)
library(magrittr)
library(phyloseq)
library(ggplot2)
library(tidyr)
```
##Setup
# Set up Paths, Directories, and Shell Variables
```{r}
#Directories
data.dir = "/data/project_data/argonne_data"
output.dir = "/home/guest/scratch/Plastics"
demux.dir = file.path(output.dir, "demux")
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)

#Files
map.file = file.path(data.dir, "200114_McCumber_16SFW_AS_200110_corrected_final.txt")
barcode.fastq = file.path(data.dir, "Undetermined_S0_L001_I1_001.fastq.gz")
r1.fastq = file.path(data.dir, "Undetermined_S0_L001_R1_001.fastq.gz")
r2.fastq = file.path(data.dir, "Undetermined_S0_L001_R2_001.fastq.gz")

plastic.map = file.path(output.dir, "plastic_metadata.tsv")
barcode.table = file.path(output.dir, "barcodes_for_fastqmultx.tsv")
rc.barcode.table = file.path(output.dir, "rc_barcodes_for_fastqmultx.tsv")

silva.ref = "/data/references/dada/silva_nr_v132_train_set.fa.gz"
silva.species.ref = "/data/references/dada/silva_species_assignment_v132.fa.gz"

ps.rds = file.path(output.dir, "plastics.rds")

#Set bash variables
Sys.setenv(MAP_FILE = map.file)
Sys.setenv(OUT_DIR = output.dir)
Sys.setenv(DEMUX_DIR = demux.dir)
Sys.setenv(RAW_FASTQ_DIR = data.dir)
Sys.setenv(R1_FASTQ = r1.fastq)
Sys.setenv(R2_FASTQ = r2.fastq)
Sys.setenv(BARCODE_FASTQ = barcode.fastq)
Sys.setenv(BARCODE_TABLE = barcode.table)
Sys.setenv(RC_BARCODE_TABLE = rc.barcode.table)
Sys.setenv(PLASTIC_MAP = plastic.map)
```

#Check Data Integrity
```{bash}
cd $RAW_FASTQ_DIR
md5sum -c md5_checksum_compressed_fastqs.txt
```
I can see that our metadata file is labeled as having been corrected from the original. Unfortunately, the md5 checksum appears to be searching for the original, so I don't know if we have a way of checking the integrity of the metadata file.

#Filter Map for Our Samples
```{r}
read_tsv(map.file) %>%
  filter(!is.na(Week)) %>%
  write_delim(plastic.map, delim = "\t", col_names = TRUE)
```

#Set up Barcode Table
```{bash}
set -u
cut --fields 1,2 $PLASTIC_MAP > $BARCODE_TABLE
```
```{bash}
set -u
head $BARCODE_TABLE
```

##Demultiplexing
#Run fastq-multx
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```
There were a lot of unmatched samples, so I need to reverse complement the barcodes.

#Reverse Complement Barcodes
```{r}
read_tsv(map.file, comment = "#q2") %>%
  select(Sample = "#SampleID", BarcodeSequence) %>%
  deframe %>%
  DNAStringSet %>%
  reverseComplement %>%
  as.data.frame() %>%
  rownames_to_column %>%
  write_delim(rc.barcode.table, delim = "\t", col_names = FALSE)
```

```{bash}
set -u
head $RC_BARCODE_TABLE
```

#Clean up the previous demultiplexing effort
```{r}
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)
```

#Run Demux with Reverse Complement Barcodes
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $RC_BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```
There were even more unmatched samples this time, so I think we should use the first run.

#Clean up the previous demultiplexing effort
```{r}
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)
```

#Rerun the first demultiplexing attempt
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```

##Filter and Trim
#Get Lists of Forward and Reverse Reads
```{r}
fnFs <- sort(list.files(demux.dir, pattern="_.+.forward.fastq", full.names = TRUE))
fnRs <- sort(list.files(demux.dir, pattern="_.+.reverse.fastq", full.names = TRUE))

forward_fastq_suffix = ".forward.fastq.gz"

fnFs %>% 
  basename %>%
  str_replace(forward_fastq_suffix,"") ->
  sample.names

sample.names
```
```{r}
print(fnFs)
```
```{r}
print(fnRs)
```
```{r}
print(sample.names)
```

#Examine Quality Profiles
```{r}
list.files(data.dir)
```

```{r}
plotQualityProfile(fnFs[1:2])
```
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R1_001.fastq.gz"))
```

```{r}
plotQualityProfile(fnRs[1:2])
```
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R2_001.fastq.gz"))
```
Overall, it looks like I can get away with trimming at 10 and 145 for both the forward and reverse reads.

#Assign filepaths for filtered files
```{r}
filt_path <- file.path(output.dir, "filtered") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```

#Filter Reads
```{r}
filt.out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=10, truncLen=c(145,145),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE) # On Windows set multithread=FALSE
```

```{r}
head(filt.out)
```

#Learn Error Rates
```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

```{r}
plotErrors(errF, nominalQ = TRUE)
```

#Dereplication
```{r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

##Sample Inference
#Infer Sequence Variants
```{r}
dadaFs <- dada(derepFs, err=errF, multithread = TRUE)
dadaRs <- dada(derepRs, err=errR, multithread = TRUE)
```

```{r}
dadaFs[[2]]
```

#Merge Paired Reads
```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
```

```{r}
head(mergers[[2]])
```

##Further Processing
#Construct Sequence Table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
table(nchar(getSequences(seqtab)))
```

```{r}
seqtab2 <- seqtab[, nchar(colnames(seqtab)) %in% seq(200,256)]
```

#Remove Chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab2)
```

#Track reads throughout the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
filt.out %>%
  as_tibble(rownames = "filename") %>%
  mutate(sample = str_replace(filename, forward_fastq_suffix, "")) %>%
  select(sample, input = reads.in, filtered = reads.out) ->
  track

sapply(dadaFs, getN) %>%
  enframe(name = "sample", value = "denoised") ->
  denoised
track %<>% full_join(denoised, by = c("sample"))

sapply(mergers, getN) %>%
  enframe(name = "sample", value = "merged") ->
  merged
track %<>% full_join(merged, by = c("sample"))

rowSums(seqtab2) %>%
  enframe(name = "sample", value = "tabled") ->
  tabled
track %<>% full_join(tabled, by = c("sample"))

rowSums(seqtab.nochim) %>%
  enframe(name = "sample", value = "nonchim") ->
  nonchim
track %<>% full_join(nonchim, by = c("sample"))

track
```

```{r}
track %>%
  gather(key = "stage", value = "counts", -c("sample")) %>%
  replace_na(list(counts = 0)) %>%
  mutate(stage=factor(stage, levels = c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim'))) %>%
  ggplot(mapping = aes(x = stage, y = counts, by = sample, group = sample)) + geom_line(alpha = 0.5) + theme_classic()
```

#Assign Taxonomy
```{r}
taxa <- assignTaxonomy(seqtab.nochim, silva.ref, multithread = TRUE)
taxa <- addSpecies(taxa, silva.species.ref)
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
```

##Make Phyloseq Object
#Load Metadata
```{r}
metadata.df = read_tsv(plastic.map, comment = "#q2") %>%
  dplyr::rename(Sample = "#SampleID") %>%
  column_to_rownames("Sample") %>%
  as.data.frame()

metadata.df
```

#Construct Phyloseq Object
```{r}
otus = otu_table(seqtab.nochim, taxa_are_rows = FALSE)
sd = sample_data(metadata.df)
ps <- phyloseq(otus, sd, tax_table(taxa))

ps
```

#Save phyloseq object as RDS
```{r}
write_rds(ps, ps.rds)
```

#Confirm that the RDS is usable
```{r}
loaded.ps = read_rds(ps.rds)
print(loaded.ps)
```
