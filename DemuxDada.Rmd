---
title: "Demultiplexing and DADA2"
author: "Katherine Mueller"
date: "3/3/2021"
output: html_document
---

# Load Libraries
```{r}
library(readr)
library(fs)
library(dplyr)
library(tibble)
library(Biostrings)
library(dada2)
library(stringr)
library(magrittr)
library(phyloseq)
library(ggplot2)
library(tidyr)
```
##Setup
# Set up Paths, Directories, and Shell Variables
```{r}
#Directories
data.dir = "/data/project_data/argonne_data"
output.dir = "/home/guest/scratch/Plastics"
demux.dir = file.path(output.dir, "demux")
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)

#Files
map.file = file.path(data.dir, "200114_McCumber_16SFW_AS_200110_corrected_final.txt")
barcode.fastq = file.path(data.dir, "Undetermined_S0_L001_I1_001.fastq.gz")
r1.fastq = file.path(data.dir, "Undetermined_S0_L001_R1_001.fastq.gz")
r2.fastq = file.path(data.dir, "Undetermined_S0_L001_R2_001.fastq.gz")

plastic.map = file.path(output.dir, "plastic_metadata.tsv")
barcode.table = file.path(output.dir, "barcodes_for_fastqmultx.tsv")
rc.barcode.table = file.path(output.dir, "rc_barcodes_for_fastqmultx.tsv")

silva.ref = "/data/references/dada/silva_nr_v132_train_set.fa.gz"
silva.species.ref = "/data/references/dada/silva_species_assignment_v132.fa.gz"

#Set bash variables
Sys.setenv(MAP_FILE = map.file)
Sys.setenv(OUT_DIR = output.dir)
Sys.setenv(DEMUX_DIR = demux.dir)
Sys.setenv(RAW_FASTQ_DIR = data.dir)
Sys.setenv(R1_FASTQ = r1.fastq)
Sys.setenv(R2_FASTQ = r2.fastq)
Sys.setenv(BARCODE_FASTQ = barcode.fastq)
Sys.setenv(BARCODE_TABLE = barcode.table)
Sys.setenv(RC_BARCODE_TABLE = rc.barcode.table)
Sys.setenv(PLASTIC_MAP = plastic.map)
```

#Check Data Integrity
```{bash}
cd $RAW_FASTQ_DIR
md5sum -c md5_checksum_compressed_fastqs.txt
```
I can see that our metadata file is labeled as having been corrected from the original. Unfortunately, the md5 checksum appears to be searching for the original, so I don't know if we have a way of checking the integrity of the metadata file.

#Filter Map for Our Samples
```{r}
read_tsv(map.file) %>%
  filter(!is.na(Week)) %>%
  write_delim(plastic.map, delim = "\t", col_names = TRUE)
```

#Set up Barcode Table
```{bash}
set -u
cut --fields 1,2 $PLASTIC_MAP > $BARCODE_TABLE
```
```{bash}
set -u
head $BARCODE_TABLE
```

##Demultiplexing
#Run fastq-multx
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```
There were a lot of unmatched samples, so I need to reverse complement the barcodes.

#Reverse Complement Barcodes
```{r}
read_tsv(map.file, comment = "#q2") %>%
  select(Sample = "#SampleID", BarcodeSequence) %>%
  deframe %>%
  DNAStringSet %>%
  reverseComplement %>%
  as.data.frame() %>%
  rownames_to_column %>%
  write_delim(rc.barcode.table, delim = "\t", col_names = FALSE)
```

```{bash}
set -u
head $RC_BARCODE_TABLE
```

#Clean up the previous demultiplexing effort
```{r}
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)
```

#Run Demux with Reverse Complement Barcodes
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $RC_BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```
There were even more unmatched samples this time, so I think we should use the first run.

#Clean up the previous demultiplexing effort
```{r}
if (dir_exists(demux.dir)) {dir_delete(demux.dir)}
dir_create(demux.dir)
```

#Rerun the first demultiplexing attempt
```{bash}
set -u
fastq-multx -m 3 -d 2 -x -B $BARCODE_TABLE \
  $BARCODE_FASTQ \
  $R1_FASTQ \
  $R2_FASTQ \
  -o $DEMUX_DIR/%_I1.fastq.gz \
  -o $DEMUX_DIR/%.forward.fastq.gz \
  -o $DEMUX_DIR/%.reverse.fastq.gz
```

##Filter and Trim
#Get Lists of Forward and Reverse Reads
```{r}
fnFs <- sort(list.files(demux.dir, pattern="_.+.forward.fastq", full.names = TRUE))
fnRs <- sort(list.files(demux.dir, pattern="_.+.reverse.fastq", full.names = TRUE))

forward_fastq_suffix = ".forward.fastq.gz"

fnFs %>% 
  basename %>%
  str_replace(forward_fastq_suffix,"") ->
  sample.names

sample.names
```
```{r}
print(fnFs)
```
```{r}
print(fnRs)
```
```{r}
print(sample.names)
```

#Examine Quality Profiles
```{r}
list.files(data.dir)
```

```{r}
plotQualityProfile(fnFs[1:2])
```
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R1_001.fastq.gz"))
```

```{r}
plotQualityProfile(fnRs[1:2])
```
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R2_001.fastq.gz"))
```
Overall, it looks like I can get away with trimming at 10 and 145 for both the forward and reverse reads.

#Assign filepaths for filtered files
```{r}
filt_path <- file.path(output.dir, "filtered") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```

#Filter Reads
```{r}
filt.out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=10, truncLen=c(145,145),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE) # On Windows set multithread=FALSE
```

```{r}
head(filt.out)
```

#Learn Error Rates
```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

```{r}
plotErrors(errF, nominalQ = TRUE)
```
